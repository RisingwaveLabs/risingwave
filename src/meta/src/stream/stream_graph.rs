// Copyright 2023 RisingWave Labs
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

use std::collections::hash_map::HashMap;
use std::collections::{BTreeMap, HashSet};
use std::num::NonZeroUsize;
use std::ops::Deref;
use std::sync::{Arc, LazyLock};

use anyhow::Context;
use assert_matches::assert_matches;
use enum_as_inner::EnumAsInner;
use itertools::Itertools;
use risingwave_common::bail;
use risingwave_common::buffer::Bitmap;
use risingwave_common::catalog::{generate_internal_table_name_with_type, TableId};
use risingwave_common::hash::{ActorId, ActorMapping, ParallelUnitId};
use risingwave_pb::catalog::Table;
use risingwave_pb::meta::table_fragments::fragment::FragmentDistributionType;
use risingwave_pb::meta::table_fragments::Fragment;
use risingwave_pb::stream_plan::stream_fragment_graph::{
    StreamFragment, StreamFragmentEdge as StreamFragmentEdgeProto,
};
use risingwave_pb::stream_plan::stream_node::NodeBody;
use risingwave_pb::stream_plan::{
    agg_call_state, DispatchStrategy, Dispatcher, DispatcherType, MergeNode, StreamActor,
    StreamFragmentGraph as StreamFragmentGraphProto, StreamNode,
};

use self::schedule::Distribution;
use super::Locations;
use crate::manager::{
    IdCategory, IdCategoryType, IdGeneratorManager, IdGeneratorManagerRef, StreamingClusterInfo,
    StreamingJob,
};
use crate::model::{DispatcherId, FragmentId};
use crate::storage::MetaStore;
use crate::MetaResult;

mod schedule;

/// A wrapper to distinguish global ID generated by the [`IdGeneratorManager`] and the local ID from
/// the frontend.
#[derive(Clone, Copy, Debug, Hash, Eq, PartialEq, PartialOrd, Ord)]
struct GlobalId<const TYPE: IdCategoryType>(u32);

impl<const TYPE: IdCategoryType> GlobalId<TYPE> {
    pub const fn new(id: u32) -> Self {
        Self(id)
    }

    pub fn as_global_id(&self) -> u32 {
        self.0
    }
}

/// Utility for converting local IDs into pre-allocated global IDs by adding an `offset`.
///
/// This requires the local IDs exactly a permutation of the range `[0, len)`.
#[derive(Clone, Copy, Debug)]
struct GlobalIdGen<const TYPE: IdCategoryType> {
    offset: u32,
    len: u32,
}

impl<const TYPE: IdCategoryType> GlobalIdGen<TYPE> {
    /// Pre-allocate a range of IDs with the given `len` and return the generator.
    pub async fn new<S: MetaStore>(id_gen: &IdGeneratorManager<S>, len: u64) -> MetaResult<Self> {
        let offset = id_gen.generate_interval::<TYPE>(len).await?;
        Ok(Self {
            offset: offset as u32,
            len: len as u32,
        })
    }

    /// Convert local id to global id. Panics if `id >= len`.
    pub fn to_global_id(self, local_id: u32) -> GlobalId<TYPE> {
        assert!(
            local_id < self.len,
            "id {} is out of range (len: {})",
            local_id,
            self.len
        );
        GlobalId(local_id + self.offset)
    }
}

type GlobalFragmentId = GlobalId<{ IdCategory::Fragment }>;
type GlobalFragmentIdGen = GlobalIdGen<{ IdCategory::Fragment }>;

type GlobalTableIdGen = GlobalIdGen<{ IdCategory::Table }>;

type GlobalActorId = GlobalId<{ IdCategory::Actor }>;
type GlobalActorIdGen = GlobalIdGen<{ IdCategory::Actor }>;

/// A list of actor IDs.
#[derive(Debug, Clone)]
struct GlobalActorIds(pub Vec<GlobalActorId>);

impl GlobalActorIds {
    pub fn as_global_ids(&self) -> Vec<u32> {
        self.0.iter().map(|x| x.as_global_id()).collect()
    }
}

/// The upstream information of an actor during the building process. This will eventually be used
/// to create the `MergeNode`s as the leaf executor of each actor.
#[derive(Debug, Clone)]
struct StreamActorUpstream {
    /// The ID of this edge.
    edge_id: EdgeId,

    /// Upstream actors.
    actors: GlobalActorIds,

    /// The fragment ID of this upstream.
    fragment_id: GlobalFragmentId,
}

/// [`StreamActorBuilder`] builds a stream actor in a stream DAG.
#[derive(Debug)]
struct StreamActorBuilder {
    /// The ID of this actor.
    actor_id: GlobalActorId,

    /// The fragment ID of this actor.
    fragment_id: GlobalFragmentId,

    /// The body of this actor, verbatim from the frontend.
    ///
    /// This cannot be directly used for execution, and it will be rewritten after we know all of
    /// the upstreams and downstreams in the end. See `rewrite`.
    nodes: Arc<StreamNode>,

    /// The dispatchers to the downstream actors.
    downstreams: HashMap<DispatcherId, Dispatcher>,

    /// The upstream actors.
    upstreams: HashMap<EdgeId, StreamActorUpstream>,

    /// The virtual node bitmap, if this fragment is hash distributed.
    vnode_bitmap: Option<Bitmap>,
}

impl StreamActorBuilder {
    pub fn new(
        actor_id: GlobalActorId,
        fragment_id: GlobalFragmentId,
        vnode_bitmap: Option<Bitmap>,
        node: Arc<StreamNode>,
    ) -> Self {
        Self {
            actor_id,
            fragment_id,
            nodes: node,
            downstreams: HashMap::new(),
            upstreams: HashMap::new(),
            vnode_bitmap,
        }
    }

    pub fn fragment_id(&self) -> GlobalFragmentId {
        self.fragment_id
    }

    /// Add a dispatcher to this actor.
    pub fn add_dispatcher(&mut self, dispatcher: Dispatcher) {
        self.downstreams
            .try_insert(dispatcher.dispatcher_id, dispatcher)
            .unwrap();
    }

    /// Add an upstream to this actor.
    pub fn add_upstream(&mut self, upstream: StreamActorUpstream) {
        self.upstreams
            .try_insert(upstream.edge_id, upstream)
            .unwrap();
    }

    /// Rewrite the actor body.
    ///
    /// During this process, the following things will be done:
    /// 1. Replace the logical `Exchange` in node's input with `Merge`, which can be executed on the
    /// compute nodes.
    /// 2. Fill the upstream mview info of the `Merge` node under the `Chain` node.
    fn rewrite(&self) -> MetaResult<StreamNode> {
        self.rewrite_inner(&self.nodes, 0)
    }

    fn rewrite_inner(&self, stream_node: &StreamNode, depth: usize) -> MetaResult<StreamNode> {
        match stream_node.get_node_body()? {
            // Leaf node `Exchange`.
            NodeBody::Exchange(exchange) => {
                // The exchange node should always be the bottom of the plan node. If we find one
                // when the depth is 0, it means that the plan node is not well-formed.
                if depth == 0 {
                    bail!(
                        "there should be no ExchangeNode on the top of the plan node: {:#?}",
                        stream_node
                    )
                }
                assert!(!stream_node.get_fields().is_empty());
                assert!(stream_node.input.is_empty());

                // Index the upstreams by the an internal edge ID.
                let upstreams = &self.upstreams[&EdgeId::Internal {
                    link_id: stream_node.get_operator_id(),
                }];

                Ok(StreamNode {
                    node_body: Some(NodeBody::Merge(MergeNode {
                        upstream_actor_id: upstreams.actors.as_global_ids(),
                        upstream_fragment_id: upstreams.fragment_id.as_global_id(),
                        upstream_dispatcher_type: exchange.get_strategy()?.r#type,
                        fields: stream_node.get_fields().clone(),
                    })),
                    identity: "MergeExecutor".to_string(),
                    ..stream_node.clone()
                })
            }

            // "Leaf" node `Chain`.
            NodeBody::Chain(chain_node) => {
                let input = stream_node.get_input();
                assert_eq!(input.len(), 2);

                let merge_node = &input[0];
                assert_matches!(merge_node.node_body, Some(NodeBody::Merge(_)));
                let batch_plan_node = &input[1];
                assert_matches!(batch_plan_node.node_body, Some(NodeBody::BatchPlan(_)));

                // Index the upstreams by the an external edge ID.
                let upstreams = &self.upstreams[&EdgeId::UpstreamExternal {
                    upstream_table_id: chain_node.table_id,
                    downstream_fragment_id: self.fragment_id,
                }];

                // As we always use the `NoShuffle` exchange for MV on MV, there should be only one
                // upstream.
                let upstream_actor_id = upstreams.actors.as_global_ids();
                assert_eq!(upstream_actor_id.len(), 1);

                let chain_input = vec![
                    // Fill the merge node with correct upstream info.
                    StreamNode {
                        input: vec![],
                        stream_key: merge_node.stream_key.clone(),
                        node_body: Some(NodeBody::Merge(MergeNode {
                            upstream_actor_id,
                            upstream_fragment_id: upstreams.fragment_id.as_global_id(),
                            upstream_dispatcher_type: DispatcherType::NoShuffle as _,
                            fields: chain_node.upstream_fields.clone(),
                        })),
                        fields: chain_node.upstream_fields.clone(),
                        operator_id: merge_node.operator_id,
                        identity: "MergeExecutor".to_string(),
                        append_only: stream_node.append_only,
                    },
                    batch_plan_node.clone(),
                ];

                Ok(StreamNode {
                    input: chain_input,
                    identity: "ChainExecutor".to_string(),
                    fields: chain_node.upstream_fields.clone(),
                    ..stream_node.clone()
                })
            }

            // For other nodes, visit the children recursively.
            _ => {
                let mut new_stream_node = stream_node.clone();
                for (input, new_input) in
                    stream_node.input.iter().zip_eq(&mut new_stream_node.input)
                {
                    *new_input = self.rewrite_inner(input, depth + 1)?;
                }
                Ok(new_stream_node)
            }
        }
    }

    /// Build an actor after all the upstreams and downstreams are processed.
    pub fn build(self, job: &StreamingJob) -> MetaResult<StreamActor> {
        let rewritten_nodes = self.rewrite()?;

        // TODO: store each upstream separately
        let upstream_actor_id = self
            .upstreams
            .into_values()
            .flat_map(|StreamActorUpstream { actors, .. }| actors.0)
            .map(|x| x.as_global_id())
            .collect();

        Ok(StreamActor {
            actor_id: self.actor_id.as_global_id(),
            fragment_id: self.fragment_id.as_global_id(),
            nodes: Some(rewritten_nodes),
            dispatcher: self.downstreams.into_values().collect(),
            upstream_actor_id,
            vnode_bitmap: self.vnode_bitmap.map(|b| b.to_protobuf()),
            mview_definition: job.mview_definition(),
        })
    }
}

/// The required changes to an existing external actor to build the graph of a streaming job.
///
/// For example, when we're creating an mview on an existing mview, we need to add new downstreams
/// to the upstream actors, by adding new dispatchers.
#[derive(Default)]
struct ExternalChange {
    /// The new downstreams to be added.
    new_downstreams: HashMap<DispatcherId, Dispatcher>,
}

impl ExternalChange {
    /// Add a dispatcher to the external actor.
    pub fn add_dispatcher(&mut self, dispatcher: Dispatcher) {
        self.new_downstreams
            .try_insert(dispatcher.dispatcher_id, dispatcher)
            .unwrap();
    }
}

/// The parallel unit location of actors.
type ActorLocations = BTreeMap<GlobalActorId, ParallelUnitId>;

/// The actual mutable state of building an actor graph.
///
/// When the fragments are visited in a topological order, actor builders will be added to this
/// state and the scheduled locations will be added. As the building process is run on the
/// **complete graph** which also contains the info of the existing (external) fragments, the info
/// of them will be also recorded.
#[derive(Default)]
struct ActorGraphBuildStateInner {
    /// The builders of the actors to be built.
    actor_builders: BTreeMap<GlobalActorId, StreamActorBuilder>,

    /// The scheduled locations of the actors to be built.
    building_locations: ActorLocations,

    /// The required changes to the external actors. See [`ExternalChange`].
    external_changes: BTreeMap<GlobalActorId, ExternalChange>,

    /// The actual locations of the external actors.
    external_locations: ActorLocations,
}

/// The information of a fragment, used for parameter passing for `Inner::add_link`.
struct FragmentLinkNode<'a> {
    fragment_id: GlobalFragmentId,
    actor_ids: &'a [GlobalActorId],
    distribution: &'a Distribution,
}

impl ActorGraphBuildStateInner {
    /// Insert new generated actor and record its location.
    ///
    /// The `vnode_bitmap` should be `Some` for the actors of hash-distributed fragments.
    pub fn add_actor(
        &mut self,
        actor_id: GlobalActorId,
        fragment_id: GlobalFragmentId,
        parallel_unit_id: ParallelUnitId,
        vnode_bitmap: Option<Bitmap>,
        node: Arc<StreamNode>,
    ) {
        self.actor_builders
            .try_insert(
                actor_id,
                StreamActorBuilder::new(actor_id, fragment_id, vnode_bitmap, node),
            )
            .unwrap();

        self.building_locations
            .try_insert(actor_id, parallel_unit_id)
            .unwrap();
    }

    /// Record the location of an external actor.
    fn record_external_location(
        &mut self,
        actor_id: GlobalActorId,
        parallel_unit_id: ParallelUnitId,
    ) {
        self.external_locations
            .try_insert(actor_id, parallel_unit_id)
            .unwrap();
    }

    /// Create a new hash dispatcher.
    fn new_hash_dispatcher(
        column_indices: &[u32],
        downstream_fragment_id: GlobalFragmentId,
        downstream_actors: &[GlobalActorId],
        downstream_actor_mapping: ActorMapping,
    ) -> Dispatcher {
        Dispatcher {
            r#type: DispatcherType::Hash as _,
            column_indices: column_indices.to_vec(),
            hash_mapping: Some(downstream_actor_mapping.to_protobuf()),
            dispatcher_id: downstream_fragment_id.as_global_id() as u64,
            downstream_actor_id: GlobalActorIds(downstream_actors.to_vec()).as_global_ids(),
        }
    }

    /// Create a new dispatcher for non-hash types.
    fn new_normal_dispatcher(
        dispatcher_type: DispatcherType,
        downstream_fragment_id: GlobalFragmentId,
        downstream_actors: &[GlobalActorId],
    ) -> Dispatcher {
        assert_ne!(dispatcher_type, DispatcherType::Hash);
        Dispatcher {
            r#type: dispatcher_type as _,
            column_indices: Vec::new(),
            hash_mapping: None,
            dispatcher_id: downstream_fragment_id.as_global_id() as u64,
            downstream_actor_id: GlobalActorIds(downstream_actors.to_vec()).as_global_ids(),
        }
    }

    /// Add the new dispatcher for an actor.
    ///
    /// - If the actor is to be built, the dispatcher will be added to the actor builder.
    /// - If the actor is an external actor, the dispatcher will be added to the external changes.
    pub fn add_dispatcher(&mut self, actor_id: GlobalActorId, dispatcher: Dispatcher) {
        if let Some(actor_builder) = self.actor_builders.get_mut(&actor_id) {
            actor_builder.add_dispatcher(dispatcher);
        } else {
            self.external_changes
                .entry(actor_id)
                .or_default()
                .add_dispatcher(dispatcher);
        }
    }

    /// Add the new upstream for an actor.
    ///
    /// - If the actor is to be built, the upstream will be added to the actor builder.
    /// - Currently there is no case that an upstream is added to an external actor.
    pub fn add_upstream(&mut self, actor_id: GlobalActorId, upstream: StreamActorUpstream) {
        if let Some(actor_builder) = self.actor_builders.get_mut(&actor_id) {
            actor_builder.add_upstream(upstream);
        } else {
            unreachable!()
        }
    }

    /// Get the location of an actor. Will look up the location map of both the actors to be built
    /// and the external actors.
    fn get_location(&self, actor_id: GlobalActorId) -> ParallelUnitId {
        self.building_locations
            .get(&actor_id)
            .copied()
            .or_else(|| self.external_locations.get(&actor_id).copied())
            .unwrap()
    }

    /// Add a "link" between two fragments in the graph.
    ///
    /// The `edge` will be expanded into multiple (downstream - upstream) pairs for the actors in
    /// the two fragments, based on the distribution and the dispatch strategy. They will be
    /// finally transformed to `Dispatcher` and `Merge` nodes when building the actors.
    ///
    /// If there're existing (external) fragments, the info will be recorded in `external_changes`,
    /// instead of the actor builders.
    pub fn add_link<'a>(
        &mut self,
        upstream: FragmentLinkNode<'a>,
        downstream: FragmentLinkNode<'a>,
        edge: &'a StreamFragmentEdge,
    ) {
        let dt = edge.dispatch_strategy.r#type();

        match dt {
            // For `NoShuffle`, make n "1-1" links between the actors.
            DispatcherType::NoShuffle => {
                for (upstream_id, downstream_id) in upstream
                    .actor_ids
                    .iter()
                    .zip_eq(downstream.actor_ids.iter())
                {
                    // Assert that the each actor pair is in the same location.
                    let upstream_location = self.get_location(*upstream_id);
                    let downstream_location = self.get_location(*downstream_id);
                    assert_eq!(upstream_location, downstream_location);

                    // Create a new dispatcher just between these two actors.
                    self.add_dispatcher(
                        *upstream_id,
                        Self::new_normal_dispatcher(dt, downstream.fragment_id, &[*downstream_id]),
                    );

                    // Also record the upstream for the downstream actor.
                    self.add_upstream(
                        *downstream_id,
                        StreamActorUpstream {
                            edge_id: edge.id,
                            actors: GlobalActorIds(vec![*upstream_id]),
                            fragment_id: upstream.fragment_id,
                        },
                    );
                }
            }

            // Otherwise, make m * n links between the actors.
            DispatcherType::Hash | DispatcherType::Broadcast | DispatcherType::Simple => {
                // Add dispatchers for the upstream actors.
                let dispatcher = if let DispatcherType::Hash = dt {
                    // Transform the `ParallelUnitMapping` from the downstream distribution to the
                    // `ActorMapping`, used for the `HashDispatcher` for the upstream actors.
                    let downstream_locations: HashMap<ParallelUnitId, ActorId> = downstream
                        .actor_ids
                        .iter()
                        .map(|&actor_id| (self.get_location(actor_id), actor_id.as_global_id()))
                        .collect();
                    let actor_mapping = downstream
                        .distribution
                        .as_hash()
                        .unwrap()
                        .to_actor(&downstream_locations);

                    Self::new_hash_dispatcher(
                        &edge.dispatch_strategy.column_indices,
                        downstream.fragment_id,
                        downstream.actor_ids,
                        actor_mapping,
                    )
                } else {
                    Self::new_normal_dispatcher(dt, downstream.fragment_id, downstream.actor_ids)
                };
                for upstream_id in upstream.actor_ids {
                    self.add_dispatcher(*upstream_id, dispatcher.clone());
                }

                // Add upstreams for the downstream actors.
                let actor_upstream = StreamActorUpstream {
                    edge_id: edge.id,
                    actors: GlobalActorIds(upstream.actor_ids.to_vec()),
                    fragment_id: upstream.fragment_id,
                };
                for downstream_id in downstream.actor_ids {
                    self.add_upstream(*downstream_id, actor_upstream.clone());
                }
            }

            DispatcherType::Unspecified => unreachable!(),
        }
    }
}

/// The mutable state of building an actor graph. See [`ActorGraphBuildStateInner`].
struct ActorGraphBuildState {
    /// The actual state.
    inner: ActorGraphBuildStateInner,

    /// The actor IDs of each fragment.
    fragment_actors: HashMap<GlobalFragmentId, Vec<GlobalActorId>>,

    /// The next local actor id to use.
    next_local_id: u32,

    /// The global actor id generator.
    actor_id_gen: GlobalActorIdGen,
}

impl ActorGraphBuildState {
    /// Create an empty state with the given id generator.
    fn new(actor_id_gen: GlobalActorIdGen) -> Self {
        Self {
            inner: Default::default(),
            fragment_actors: Default::default(),
            next_local_id: 0,
            actor_id_gen,
        }
    }

    /// Get the next global actor id.
    fn next_actor_id(&mut self) -> GlobalActorId {
        let local_id = self.next_local_id;
        self.next_local_id += 1;

        self.actor_id_gen.to_global_id(local_id)
    }

    /// Finish the build and return the inner state.
    fn finish(self) -> ActorGraphBuildStateInner {
        // Assert that all the actors are built.
        assert_eq!(self.actor_id_gen.len, self.next_local_id);

        self.inner
    }
}

/// The result of a built actor graph. Will be further embedded into the `Context` for building
/// actors on the compute nodes.
pub struct ActorGraphBuildResult {
    /// The graph of sealed fragments, including all actors.
    pub graph: BTreeMap<FragmentId, Fragment>,

    /// The scheduled locations of the actors to be built.
    pub building_locations: Locations,

    /// The actual locations of the external actors.
    pub existing_locations: Locations,

    /// The new dispatchers to be added to the upstream mview actors.
    pub dispatchers: HashMap<ActorId, Vec<Dispatcher>>,
}

/// [`ActorGraphBuilder`] builds the actor graph for the given complete fragment graph, based on the
/// current cluster info and the required parallelism.
pub struct ActorGraphBuilder {
    /// The pre-scheduled distribution for each building fragment.
    distributions: HashMap<GlobalFragmentId, Distribution>,

    /// The complete fragment graph.
    fragment_graph: CompleteStreamFragmentGraph,

    /// The cluster info for creating a streaming job.
    cluster_info: StreamingClusterInfo,
}

impl ActorGraphBuilder {
    /// Create a new actor graph builder with the given "complete" graph. Returns an error if the
    /// graph is failed to be scheduled.
    pub fn new(
        fragment_graph: CompleteStreamFragmentGraph,
        cluster_info: StreamingClusterInfo,
        default_parallelism: Option<NonZeroUsize>,
    ) -> MetaResult<Self> {
        // Schedule the distribution of all building fragments.
        let distributions = schedule::Scheduler::new(
            cluster_info.parallel_units.values().cloned(),
            default_parallelism,
        )?
        .schedule(&fragment_graph)?;

        Ok(Self {
            distributions,
            fragment_graph,
            cluster_info,
        })
    }

    /// Convert the actor location map to the [`Locations`] struct.
    fn build_locations(&self, actor_locations: ActorLocations) -> Locations {
        let actor_locations = actor_locations
            .into_iter()
            .map(|(id, p)| {
                (
                    id.as_global_id(),
                    self.cluster_info.parallel_units[&p].clone(),
                )
            })
            .collect();

        let worker_locations = self.cluster_info.worker_nodes.clone();

        Locations {
            actor_locations,
            worker_locations,
        }
    }

    /// Build a stream graph by duplicating each fragment as parallel actors. Returns
    /// [`ActorGraphBuildResult`] that will be further used to build actors on the compute nodes.
    pub async fn generate_graph<S>(
        self,
        id_gen_manager: IdGeneratorManagerRef<S>,
        job: &StreamingJob,
    ) -> MetaResult<ActorGraphBuildResult>
    where
        S: MetaStore,
    {
        // Pre-generate IDs for all actors.
        let actor_len = self
            .distributions
            .values()
            .map(|d| d.parallelism())
            .sum::<usize>() as u64;
        let id_gen = GlobalActorIdGen::new(&id_gen_manager, actor_len).await?;

        // Build the actor graph and get the final state.
        let ActorGraphBuildStateInner {
            actor_builders,
            building_locations,
            external_changes,
            external_locations,
        } = self.build_actor_graph(id_gen)?;

        // Serialize the graph into a map of sealed fragments.
        let graph = {
            let mut actors: HashMap<GlobalFragmentId, Vec<StreamActor>> = HashMap::new();

            // As all fragments are processed, we can now `build` the actors where the `Exchange`
            // and `Chain` are rewritten.
            for builder in actor_builders.into_values() {
                let fragment_id = builder.fragment_id();
                let actor = builder.build(job)?;
                actors.entry(fragment_id).or_default().push(actor);
            }

            actors
                .into_iter()
                .map(|(fragment_id, actors)| {
                    let distribution = self.distributions[&fragment_id].clone();
                    let fragment =
                        self.fragment_graph
                            .seal_fragment(fragment_id, actors, distribution);
                    let fragment_id = fragment_id.as_global_id();
                    (fragment_id, fragment)
                })
                .collect()
        };

        // Convert the actor location map to the `Locations` struct.
        let building_locations = self.build_locations(building_locations);
        let existing_locations = self.build_locations(external_locations);

        // Extract the new dispatchers from the external changes.
        let dispatchers = external_changes
            .iter()
            .map(|(actor_id, change)| {
                (
                    actor_id.as_global_id(),
                    change.new_downstreams.values().cloned().collect(),
                )
            })
            .collect();

        Ok(ActorGraphBuildResult {
            graph,
            building_locations,
            existing_locations,
            dispatchers,
        })
    }

    /// Build actor graph for each fragment, using topological order.
    fn build_actor_graph(&self, id_gen: GlobalActorIdGen) -> MetaResult<ActorGraphBuildStateInner> {
        let mut state = ActorGraphBuildState::new(id_gen);

        // Use topological sort to build the graph from downstream to upstream. (The first fragment
        // popped out from the heap will be the top-most node in plan, or the sink in stream graph.)
        for fragment_id in self.fragment_graph.topo_order()? {
            self.build_actor_graph_fragment(fragment_id, &mut state)?;
        }

        Ok(state.finish())
    }

    /// Build actor graph for a specific fragment.
    fn build_actor_graph_fragment(
        &self,
        fragment_id: GlobalFragmentId,
        state: &mut ActorGraphBuildState,
    ) -> MetaResult<()> {
        let current_fragment = self.fragment_graph.get_fragment(fragment_id);

        // First, add or record the actors for the current fragment into the state.
        let (distribution, actor_ids) = match current_fragment {
            // For building fragments, we need to generate the actor builders.
            EitherFragment::Building(current_fragment) => {
                let node = Arc::new(current_fragment.node.clone().unwrap());
                let distribution = self.distributions[&fragment_id].clone();
                let bitmaps = distribution.as_hash().map(|m| m.to_bitmaps());

                let actor_ids = distribution
                    .parallel_units()
                    .map(|parallel_unit_id| {
                        let actor_id = state.next_actor_id();
                        let vnode_bitmap = bitmaps.as_ref().map(|m| &m[&parallel_unit_id]).cloned();

                        state.inner.add_actor(
                            actor_id,
                            fragment_id,
                            parallel_unit_id,
                            vnode_bitmap,
                            node.clone(),
                        );

                        actor_id
                    })
                    .collect_vec();

                (distribution, actor_ids)
            }

            // For existing fragments, we only need to record the actor locations.
            EitherFragment::Existing(existing_fragment) => {
                let distribution = Distribution::from_fragment(&existing_fragment);

                let actor_ids = existing_fragment
                    .actors
                    .iter()
                    .map(|a| {
                        let actor_id = GlobalActorId::new(a.actor_id);
                        let parallel_unit_id = match &distribution {
                            Distribution::Singleton(parallel_unit_id) => *parallel_unit_id,
                            Distribution::Hash(mapping) => mapping
                                .get_matched(&Bitmap::from(a.get_vnode_bitmap().unwrap()))
                                .unwrap(),
                        };

                        state
                            .inner
                            .record_external_location(actor_id, parallel_unit_id);

                        actor_id
                    })
                    .collect_vec();

                (distribution, actor_ids)
            }
        };

        // Then, add links between the current fragment and its downstream fragments.
        for (downstream_fragment_id, edge) in self.fragment_graph.get_downstreams(fragment_id) {
            let downstream_actors = state
                .fragment_actors
                .get(&downstream_fragment_id)
                .expect("downstream fragment not processed yet");

            // TODO(bugen): For replacing fragments, it's possible that the downstream fragment is
            // an external one. We should also record the external distribution then.
            let downstream_distribution = &self.distributions[&downstream_fragment_id];

            state.inner.add_link(
                FragmentLinkNode {
                    fragment_id,
                    actor_ids: &actor_ids,
                    distribution: &distribution,
                },
                FragmentLinkNode {
                    fragment_id: downstream_fragment_id,
                    actor_ids: downstream_actors,
                    distribution: downstream_distribution,
                },
                edge,
            );
        }

        // Finally, record the actor IDs for the current fragment.
        state
            .fragment_actors
            .try_insert(fragment_id, actor_ids)
            .unwrap_or_else(|_| panic!("fragment {:?} is already processed", fragment_id));

        Ok(())
    }
}

/// The fragment in the building phase, including the [`StreamFragment`] from the frontend and
/// several additional helper fields.
#[derive(Debug, Clone)]
pub struct BuildingFragment {
    /// The fragment structure from the frontend, with the global fragment ID.
    inner: StreamFragment,

    /// A clone of the internal tables in this fragment.
    internal_tables: Vec<Table>,

    /// The ID of the job if it's materialized in this fragment.
    table_id: Option<u32>,
}

impl BuildingFragment {
    /// Create a new [`BuildingFragment`] from a [`StreamFragment`]. The global fragment ID and
    /// global table IDs will be correctly filled with the given `id` and `table_id_gen`.
    fn new(
        id: GlobalFragmentId,
        fragment: StreamFragment,
        job: &StreamingJob,
        table_id_gen: GlobalTableIdGen,
    ) -> Self {
        let mut fragment = StreamFragment {
            fragment_id: id.as_global_id(),
            ..fragment
        };
        let internal_tables = Self::fill_internal_tables(&mut fragment, job, table_id_gen);
        let table_id = Self::fill_job(&mut fragment, job).then(|| job.id());

        Self {
            inner: fragment,
            internal_tables,
            table_id,
        }
    }

    /// Fill the information of the internal tables in the fragment.
    fn fill_internal_tables(
        fragment: &mut StreamFragment,
        job: &StreamingJob,
        table_id_gen: GlobalTableIdGen,
    ) -> Vec<Table> {
        let fragment_id = fragment.fragment_id;
        let mut internal_tables = Vec::new();

        visit_internal_tables(fragment, |table, table_type_name| {
            table.id = table_id_gen.to_global_id(table.id).as_global_id();
            table.schema_id = job.schema_id();
            table.database_id = job.database_id();
            table.name = generate_internal_table_name_with_type(
                &job.name(),
                fragment_id,
                table.id,
                table_type_name,
            );
            table.fragment_id = fragment_id;

            // Record the internal table.
            internal_tables.push(table.clone());
        });

        internal_tables
    }

    /// Fill the information of the job in the fragment.
    fn fill_job(fragment: &mut StreamFragment, job: &StreamingJob) -> bool {
        let table_id = job.id();
        let fragment_id = fragment.fragment_id;
        let mut has_table = false;

        visit_fragment(fragment, |node_body| match node_body {
            NodeBody::Materialize(materialize_node) => {
                materialize_node.table_id = table_id;

                // Fill the ID of the `Table`.
                let table = materialize_node.table.as_mut().unwrap();
                table.id = table_id;
                table.database_id = job.database_id();
                table.schema_id = job.schema_id();
                table.fragment_id = fragment_id;

                has_table = true;
            }
            NodeBody::Sink(sink_node) => {
                sink_node.table_id = table_id;

                has_table = true;
            }
            NodeBody::Dml(dml_node) => {
                dml_node.table_id = table_id;
            }
            _ => {}
        });

        has_table
    }
}

impl Deref for BuildingFragment {
    type Target = StreamFragment;

    fn deref(&self) -> &Self::Target {
        &self.inner
    }
}

/// The ID of an edge in the fragment graph. For different types of edges, the ID will be in
/// different variants.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
enum EdgeId {
    /// The edge between two building (internal) fragments.
    Internal {
        /// The ID generated by the frontend, generally the operator ID of `Exchange`.
        /// See [`StreamFragmentEdgeProto`].
        link_id: u64,
    },

    /// The edge between an upstream external fragment and downstream building fragment. Used for
    /// MV on MV.
    UpstreamExternal {
        /// The ID of the upstream table or materialized view.
        upstream_table_id: u32,
        /// The ID of the downstream fragment.
        downstream_fragment_id: GlobalFragmentId,
    },
}

/// The edge in the fragment graph.
///
/// The edge can be either internal or external. This is distinguished by the [`EdgeId`].
#[derive(Debug, Clone)]
struct StreamFragmentEdge {
    /// The ID of the edge.
    id: EdgeId,

    /// The strategy used for dispatching the data.
    dispatch_strategy: DispatchStrategy,
}

impl StreamFragmentEdge {
    fn from_protobuf(edge: &StreamFragmentEdgeProto) -> Self {
        Self {
            // By creating an edge from the protobuf, we know that the edge is from the frontend and
            // is internal.
            id: EdgeId::Internal {
                link_id: edge.link_id,
            },
            dispatch_strategy: edge.get_dispatch_strategy().unwrap().clone(),
        }
    }
}

/// In-memory representation of a **Fragment** Graph, built from the [`StreamFragmentGraphProto`]
/// from the frontend.
#[derive(Default)]
pub struct StreamFragmentGraph {
    /// stores all the fragments in the graph.
    fragments: HashMap<GlobalFragmentId, BuildingFragment>,

    /// stores edges between fragments: upstream => downstream.
    downstreams: HashMap<GlobalFragmentId, HashMap<GlobalFragmentId, StreamFragmentEdge>>,

    /// stores edges between fragments: downstream -> upstream.
    upstreams: HashMap<GlobalFragmentId, HashMap<GlobalFragmentId, StreamFragmentEdge>>,

    /// Dependent relations of this job.
    dependent_relations: HashSet<TableId>,
}

impl StreamFragmentGraph {
    /// Create a new [`StreamFragmentGraph`] from the given [`StreamFragmentGraphProto`], with all
    /// global IDs correctly filled.
    pub async fn new<S: MetaStore>(
        proto: StreamFragmentGraphProto,
        id_gen: IdGeneratorManagerRef<S>,
        job: &StreamingJob,
    ) -> MetaResult<Self> {
        let fragment_id_gen =
            GlobalFragmentIdGen::new(&id_gen, proto.fragments.len() as u64).await?;
        let table_id_gen = GlobalTableIdGen::new(&id_gen, proto.table_ids_cnt as u64).await?;

        // Create nodes.
        let fragments: HashMap<_, _> = proto
            .fragments
            .into_iter()
            .map(|(id, fragment)| {
                let id = fragment_id_gen.to_global_id(id);
                let fragment = BuildingFragment::new(id, fragment, job, table_id_gen);
                (id, fragment)
            })
            .collect();

        assert_eq!(
            fragments
                .values()
                .map(|f| f.internal_tables.len() as u32)
                .sum::<u32>(),
            proto.table_ids_cnt
        );

        // Create edges.
        let mut downstreams = HashMap::new();
        let mut upstreams = HashMap::new();

        for edge in proto.edges {
            let upstream_id = fragment_id_gen.to_global_id(edge.upstream_id);
            let downstream_id = fragment_id_gen.to_global_id(edge.downstream_id);
            let edge = StreamFragmentEdge::from_protobuf(&edge);

            upstreams
                .entry(downstream_id)
                .or_insert_with(HashMap::new)
                .try_insert(upstream_id, edge.clone())
                .unwrap();
            downstreams
                .entry(upstream_id)
                .or_insert_with(HashMap::new)
                .try_insert(downstream_id, edge)
                .unwrap();
        }

        // Note: Here we directly use the field `dependent_table_ids` in the proto (resolved in
        // frontend), instead of visiting the graph ourselves. Note that for creating table with a
        // connector, the source itself is NOT INCLUDED in this list.
        let dependent_relations = proto
            .dependent_table_ids
            .iter()
            .map(TableId::from)
            .collect();

        Ok(Self {
            fragments,
            downstreams,
            upstreams,
            dependent_relations,
        })
    }

    /// Retrieve the internal tables map of the whole graph.
    pub fn internal_tables(&self) -> HashMap<u32, Table> {
        let mut tables = HashMap::new();
        for fragment in self.fragments.values() {
            for table in &fragment.internal_tables {
                tables
                    .try_insert(table.id, table.clone())
                    .unwrap_or_else(|_| panic!("duplicated table id `{}`", table.id));
            }
        }
        tables
    }

    /// Returns the fragment id where the table is materialized.
    pub fn table_fragment_id(&self) -> FragmentId {
        self.fragments
            .values()
            .filter(|b| b.table_id.is_some())
            .map(|b| b.fragment_id)
            .exactly_one()
            .expect("require exactly 1 materialize/sink node when creating the streaming job")
    }

    /// Get the dependent relations of this job.
    pub fn dependent_relations(&self) -> &HashSet<TableId> {
        &self.dependent_relations
    }

    /// Get downstreams of a fragment.
    fn get_downstreams(
        &self,
        fragment_id: GlobalFragmentId,
    ) -> &HashMap<GlobalFragmentId, StreamFragmentEdge> {
        self.downstreams.get(&fragment_id).unwrap_or(&EMPTY_HASHMAP)
    }

    /// Get upstreams of a fragment.
    fn get_upstreams(
        &self,
        fragment_id: GlobalFragmentId,
    ) -> &HashMap<GlobalFragmentId, StreamFragmentEdge> {
        self.upstreams.get(&fragment_id).unwrap_or(&EMPTY_HASHMAP)
    }
}

static EMPTY_HASHMAP: LazyLock<HashMap<GlobalFragmentId, StreamFragmentEdge>> =
    LazyLock::new(HashMap::new);

/// A fragment that is either being built or already exists. Used for generalize the logic of
/// [`ActorGraphBuilder`].
#[derive(Debug, Clone, EnumAsInner)]
enum EitherFragment {
    /// An internal fragment that is being built for the current streaming job.
    Building(BuildingFragment),

    /// An existing fragment that is external but connected to the fragments being built.
    Existing(Fragment),
}

/// A wrapper of [`StreamFragmentGraph`] that contains the additional information of existing
/// fragments, which is connected to the graph's top-most or bottom-most fragments.
///
/// For example, if we're going to build a mview on an existing mview, the upstream fragment
/// containing the `Materialize` node will be included in this structure.
pub struct CompleteStreamFragmentGraph {
    /// The fragment graph of the streaming job being built.
    building_graph: StreamFragmentGraph,

    /// The required information of existing fragments.
    existing_fragments: HashMap<GlobalFragmentId, Fragment>,

    /// Extra edges between existing fragments and the building fragments.
    extra_downstreams: HashMap<GlobalFragmentId, HashMap<GlobalFragmentId, StreamFragmentEdge>>,

    /// Extra edges between existing fragments and the building fragments.
    extra_upstreams: HashMap<GlobalFragmentId, HashMap<GlobalFragmentId, StreamFragmentEdge>>,
}

impl CompleteStreamFragmentGraph {
    /// Create a new [`CompleteStreamFragmentGraph`] with empty existing fragments, i.e., there's no
    /// upstream mviews.
    #[cfg(test)]
    pub fn for_test(graph: StreamFragmentGraph) -> Self {
        Self {
            building_graph: graph,
            existing_fragments: Default::default(),
            extra_downstreams: Default::default(),
            extra_upstreams: Default::default(),
        }
    }

    /// Create a new [`CompleteStreamFragmentGraph`] for MV on MV. Returns an error if the upstream
    /// `Matererialize` is failed to resolve.
    pub fn new(
        graph: StreamFragmentGraph,
        upstream_mview_fragments: HashMap<TableId, Fragment>,
    ) -> MetaResult<Self> {
        let mut extra_downstreams = HashMap::new();
        let mut extra_upstreams = HashMap::new();

        // Build the extra edges between the upstream `Materialize` and the downstream `Chain` of
        // the new materialized view.
        for (&id, fragment) in &graph.fragments {
            for &upstream_table_id in &fragment.upstream_table_ids {
                let mview_fragment = upstream_mview_fragments
                    .get(&TableId::new(upstream_table_id))
                    .context("upstream materialized view fragment not found")?;
                let mview_id = GlobalFragmentId::new(mview_fragment.fragment_id);

                let edge = StreamFragmentEdge {
                    id: EdgeId::UpstreamExternal {
                        upstream_table_id,
                        downstream_fragment_id: id,
                    },
                    // We always use `NoShuffle` for the exchange between the upstream `Materialize`
                    // and the downstream `Chain` of the new materialized view.
                    dispatch_strategy: DispatchStrategy {
                        r#type: DispatcherType::NoShuffle as _,
                        ..Default::default()
                    },
                };

                extra_downstreams
                    .entry(mview_id)
                    .or_insert_with(HashMap::new)
                    .try_insert(id, edge.clone())
                    .unwrap();
                extra_upstreams
                    .entry(id)
                    .or_insert_with(HashMap::new)
                    .try_insert(mview_id, edge)
                    .unwrap();
            }
        }

        let existing_fragments = upstream_mview_fragments
            .into_values()
            .map(|f| (GlobalFragmentId::new(f.fragment_id), f))
            .collect();

        Ok(Self {
            building_graph: graph,
            existing_fragments,
            extra_downstreams,
            extra_upstreams,
        })
    }

    /// Returns **all** fragment IDs in the complete graph, including the ones that are not in the
    /// building graph.
    fn all_fragment_ids(&self) -> impl Iterator<Item = GlobalFragmentId> + '_ {
        self.building_graph
            .fragments
            .keys()
            .chain(self.existing_fragments.keys())
            .copied()
    }

    /// Returns an iterator of **all** edges in the complete graph, including the external edges.
    fn all_edges(
        &self,
    ) -> impl Iterator<Item = (GlobalFragmentId, GlobalFragmentId, &StreamFragmentEdge)> + '_ {
        self.building_graph
            .downstreams
            .iter()
            .chain(self.extra_downstreams.iter())
            .flat_map(|(&from, tos)| tos.iter().map(move |(&to, edge)| (from, to, edge)))
    }

    /// Returns the distribution of the existing fragments.
    fn existing_distribution(&self) -> HashMap<GlobalFragmentId, Distribution> {
        self.existing_fragments
            .iter()
            .map(|(&id, f)| (id, Distribution::from_fragment(f)))
            .collect()
    }

    /// Generate topological order of **all** fragments in this graph, including the ones that are
    /// not in the building graph. Returns error if the graph is not a DAG and topological sort can
    /// not be done.
    ///
    /// For MV on MV, the first fragment popped out from the heap will be the top-most node, or the
    /// `Sink` / `Materialize` in stream graph.
    fn topo_order(&self) -> MetaResult<Vec<GlobalFragmentId>> {
        let mut topo = Vec::new();
        let mut downstream_cnts = HashMap::new();

        // Iterate all fragments.
        for fragment_id in self.all_fragment_ids() {
            // Count how many downstreams we have for a given fragment.
            let downstream_cnt = self.get_downstreams(fragment_id).count();
            if downstream_cnt == 0 {
                topo.push(fragment_id);
            } else {
                downstream_cnts.insert(fragment_id, downstream_cnt);
            }
        }

        let mut i = 0;
        while let Some(&fragment_id) = topo.get(i) {
            i += 1;
            // Find if we can process more fragments.
            for (upstream_id, _) in self.get_upstreams(fragment_id) {
                let downstream_cnt = downstream_cnts.get_mut(&upstream_id).unwrap();
                *downstream_cnt -= 1;
                if *downstream_cnt == 0 {
                    downstream_cnts.remove(&upstream_id);
                    topo.push(upstream_id);
                }
            }
        }

        if !downstream_cnts.is_empty() {
            // There are fragments that are not processed yet.
            bail!("graph is not a DAG");
        }

        Ok(topo)
    }

    /// Seal a [`BuildingFragment`] from the graph into a [`Fragment`], which will be further used
    /// to build actors on the compute nodes and persist into meta store.
    fn seal_fragment(
        &self,
        id: GlobalFragmentId,
        actors: Vec<StreamActor>,
        distribution: Distribution,
    ) -> Fragment {
        let BuildingFragment {
            inner,
            internal_tables,
            table_id,
        } = self.get_fragment(id).into_building().unwrap();

        let distribution_type = if inner.is_singleton {
            FragmentDistributionType::Single
        } else {
            FragmentDistributionType::Hash
        } as i32;

        let state_table_ids = internal_tables
            .iter()
            .map(|t| t.id)
            .chain(table_id)
            .collect();

        let upstream_fragment_ids = self
            .get_upstreams(id)
            .map(|(id, _)| id.as_global_id())
            .collect();

        Fragment {
            fragment_id: inner.fragment_id,
            fragment_type_mask: inner.fragment_type_mask,
            distribution_type,
            actors,
            vnode_mapping: Some(distribution.into_mapping().to_protobuf()),
            state_table_ids,
            upstream_fragment_ids,
        }
    }

    /// Get a fragment from the complete graph, which can be either a building fragment or an
    /// existing fragment.
    fn get_fragment(&self, fragment_id: GlobalFragmentId) -> EitherFragment {
        if let Some(fragment) = self.existing_fragments.get(&fragment_id) {
            EitherFragment::Existing(fragment.clone())
        } else {
            EitherFragment::Building(
                self.building_graph
                    .fragments
                    .get(&fragment_id)
                    .unwrap()
                    .clone(),
            )
        }
    }

    /// Get **all** downstreams of a fragment, including the ones that are not in the building
    /// graph.
    fn get_downstreams(
        &self,
        fragment_id: GlobalFragmentId,
    ) -> impl Iterator<Item = (GlobalFragmentId, &StreamFragmentEdge)> {
        self.building_graph
            .get_downstreams(fragment_id)
            .iter()
            .chain(
                self.extra_downstreams
                    .get(&fragment_id)
                    .into_iter()
                    .flatten(),
            )
            .map(|(&id, edge)| (id, edge))
    }

    /// Get **all** upstreams of a fragment, including the ones that are not in the building
    /// graph.
    fn get_upstreams(
        &self,
        fragment_id: GlobalFragmentId,
    ) -> impl Iterator<Item = (GlobalFragmentId, &StreamFragmentEdge)> {
        self.building_graph
            .get_upstreams(fragment_id)
            .iter()
            .chain(self.extra_upstreams.get(&fragment_id).into_iter().flatten())
            .map(|(&id, edge)| (id, edge))
    }

    /// Returns all building fragments in the graph.
    fn building_fragments(&self) -> &HashMap<GlobalFragmentId, BuildingFragment> {
        &self.building_graph.fragments
    }
}

/// A utility for visiting and mutating the [`NodeBody`] of the [`StreamNode`]s in a
/// [`StreamFragment`] recursively.
pub fn visit_fragment<F>(fragment: &mut StreamFragment, mut f: F)
where
    F: FnMut(&mut NodeBody),
{
    fn visit_inner<F>(stream_node: &mut StreamNode, f: &mut F)
    where
        F: FnMut(&mut NodeBody),
    {
        f(stream_node.node_body.as_mut().unwrap());
        for input in &mut stream_node.input {
            visit_inner(input, f);
        }
    }

    visit_inner(fragment.node.as_mut().unwrap(), &mut f)
}

/// Visit the internal tables of a [`StreamFragment`].
fn visit_internal_tables<F>(fragment: &mut StreamFragment, mut f: F)
where
    F: FnMut(&mut Table, &'static str),
{
    macro_rules! always {
        ($table:expr, $name:expr) => {{
            let table = $table
                .as_mut()
                .unwrap_or_else(|| panic!("internal table {} should always exist", $name));
            f(table, $name);
        }};
    }

    #[allow(unused_macros)]
    macro_rules! optional {
        ($table:expr, $name:expr) => {
            if let Some(table) = &mut $table {
                f(table, $name);
            }
        };
    }

    visit_fragment(fragment, |body| {
        match body {
            // Join
            NodeBody::HashJoin(node) => {
                // TODO: make the degree table optional
                always!(node.left_table, "HashJoinLeft");
                always!(node.left_degree_table, "HashJoinDegreeLeft");
                always!(node.right_table, "HashJoinRight");
                always!(node.right_degree_table, "HashJoinDegreeRight");
            }
            NodeBody::DynamicFilter(node) => {
                always!(node.left_table, "DynamicFilterLeft");
                always!(node.right_table, "DynamicFilterRight");
            }

            // Aggregation
            NodeBody::HashAgg(node) => {
                assert_eq!(node.agg_call_states.len(), node.agg_calls.len());
                always!(node.result_table, "HashAggResult");
                for state in &mut node.agg_call_states {
                    if let agg_call_state::Inner::MaterializedInputState(s) =
                        state.inner.as_mut().unwrap()
                    {
                        always!(s.table, "HashAgg");
                    }
                }
            }
            NodeBody::GlobalSimpleAgg(node) => {
                assert_eq!(node.agg_call_states.len(), node.agg_calls.len());
                always!(node.result_table, "GlobalSimpleAggResult");
                for state in &mut node.agg_call_states {
                    if let agg_call_state::Inner::MaterializedInputState(s) =
                        state.inner.as_mut().unwrap()
                    {
                        always!(s.table, "GlobalSimpleAgg");
                    }
                }
            }

            // Top-N
            NodeBody::AppendOnlyTopN(node) => {
                always!(node.table, "AppendOnlyTopN");
            }
            NodeBody::TopN(node) => {
                always!(node.table, "TopN");
            }
            NodeBody::AppendOnlyGroupTopN(node) => {
                always!(node.table, "AppendOnlyGroupTopN");
            }
            NodeBody::GroupTopN(node) => {
                always!(node.table, "GroupTopN");
            }

            // Source
            NodeBody::Source(node) => {
                if let Some(source) = &mut node.source_inner {
                    always!(source.state_table, "Source");
                }
            }
            NodeBody::Now(node) => {
                always!(node.state_table, "Now");
            }

            // Shared arrangement
            NodeBody::Arrange(node) => {
                always!(node.table, "Arrange");
            }

            // Note: add internal tables for new nodes here.
            _ => {}
        }
    })
}
