// Copyright 2023 RisingWave Labs
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

use std::collections::hash_map::HashMap;
use std::collections::{BTreeMap, HashSet};
use std::ops::Deref;
use std::sync::{Arc, LazyLock};

use anyhow::Context;
use assert_matches::assert_matches;
use itertools::Itertools;
use risingwave_common::bail;
use risingwave_common::catalog::{generate_internal_table_name_with_type, TableId};
use risingwave_common::hash::ParallelUnitMapping;
use risingwave_pb::catalog::Table;
use risingwave_pb::common::ParallelUnit;
use risingwave_pb::meta::table_fragments::fragment::FragmentDistributionType;
use risingwave_pb::meta::table_fragments::Fragment;
use risingwave_pb::stream_plan::stream_fragment_graph::{StreamFragment, StreamFragmentEdge};
use risingwave_pb::stream_plan::stream_node::NodeBody;
use risingwave_pb::stream_plan::{
    agg_call_state, ColocatedActorId, DispatchStrategy, Dispatcher, DispatcherType, MergeNode,
    StreamActor, StreamFragmentGraph as StreamFragmentGraphProto, StreamNode,
};

use self::schedule::Distribution;
use super::CreateStreamingJobContext;
use crate::manager::{
    IdCategory, IdCategoryType, IdGeneratorManager, IdGeneratorManagerRef, StreamingJob,
};
use crate::model::FragmentId;
use crate::storage::MetaStore;
use crate::MetaResult;

mod schedule;

/// A wrapper to distinguish global ID generated by the [`IdGeneratorManager`] and the local ID from
/// the frontend.
#[derive(Clone, Copy, Debug, Hash, Eq, PartialEq, PartialOrd, Ord)]
struct GlobalId<const TYPE: IdCategoryType>(u32);

impl<const TYPE: IdCategoryType> GlobalId<TYPE> {
    pub const fn new(id: u32) -> Self {
        Self(id)
    }

    pub fn as_global_id(&self) -> u32 {
        self.0
    }
}

/// Utility for converting local IDs into pre-allocated global IDs by adding an `offset`.
///
/// This requires the local IDs exactly a permutation of the range `[0, len)`.
#[derive(Clone, Copy, Debug)]
struct GlobalIdGen<const TYPE: IdCategoryType> {
    offset: u32,
    len: u32,
}

impl<const TYPE: IdCategoryType> GlobalIdGen<TYPE> {
    /// Pre-allocate a range of IDs with the given `len` and return the generator.
    pub async fn new<S: MetaStore>(id_gen: &IdGeneratorManager<S>, len: u64) -> MetaResult<Self> {
        let offset = id_gen.generate_interval::<TYPE>(len).await?;
        Ok(Self {
            offset: offset as u32,
            len: len as u32,
        })
    }

    /// Convert local id to global id. Panics if `id >= len`.
    pub fn to_global_id(self, local_id: u32) -> GlobalId<TYPE> {
        assert!(
            local_id < self.len,
            "id {} is out of range (len: {})",
            local_id,
            self.len
        );
        GlobalId(local_id + self.offset)
    }
}

type GlobalFragmentId = GlobalId<{ IdCategory::Fragment }>;
type GlobalFragmentIdGen = GlobalIdGen<{ IdCategory::Fragment }>;

type GlobalTableIdGen = GlobalIdGen<{ IdCategory::Table }>;

type GlobalActorId = GlobalId<{ IdCategory::Actor }>;
type GlobalActorIdGen = GlobalIdGen<{ IdCategory::Actor }>;

/// A list of actors with order.
#[derive(Debug, Clone)]
struct OrderedActorLink(pub Vec<GlobalActorId>);

impl OrderedActorLink {
    pub fn as_global_ids(&self) -> Vec<u32> {
        self.0.iter().map(|x| x.as_global_id()).collect()
    }
}

struct StreamActorDownstream {
    dispatch_strategy: DispatchStrategy,
    dispatcher_id: u64,

    /// Downstream actors.
    actors: OrderedActorLink,

    /// Whether to place the downstream actors on the same node
    // TODO: remove this field after scheduler refactoring
    #[expect(dead_code)]
    same_worker_node: bool,
}

#[derive(Debug)]
struct StreamActorUpstream {
    /// Upstream actors
    actors: OrderedActorLink,
    /// associate fragment id
    fragment_id: GlobalFragmentId,
    /// Whether to place the upstream actors on the same node
    same_worker_node: bool,
}

/// [`StreamActorBuilder`] builds a stream actor in a stream DAG.
struct StreamActorBuilder {
    /// actor id field
    actor_id: GlobalActorId,

    /// associated fragment id
    fragment_id: GlobalFragmentId,

    /// associated stream node
    nodes: Arc<StreamNode>,

    /// downstream dispatchers (dispatcher, downstream actor, hash mapping)
    downstreams: Vec<StreamActorDownstream>,

    /// upstreams, exchange node operator_id -> upstream actor ids
    upstreams: HashMap<u64, StreamActorUpstream>,

    /// Whether to place this actors on the same node as chain's upstream MVs.
    chain_same_worker_node: bool,
}

impl StreamActorBuilder {
    fn is_chain_same_worker_node(stream_node: &StreamNode) -> bool {
        fn visit(stream_node: &StreamNode) -> bool {
            if let Some(NodeBody::Chain(ref chain)) = stream_node.node_body {
                return chain.same_worker_node;
            }
            stream_node.input.iter().any(visit)
        }
        visit(stream_node)
    }

    pub fn new(
        actor_id: GlobalActorId,
        fragment_id: GlobalFragmentId,
        node: Arc<StreamNode>,
    ) -> Self {
        Self {
            actor_id,
            fragment_id,
            chain_same_worker_node: Self::is_chain_same_worker_node(&node),
            nodes: node,
            downstreams: vec![],
            upstreams: HashMap::new(),
        }
    }

    pub fn fragment_id(&self) -> GlobalFragmentId {
        self.fragment_id
    }

    /// Add a dispatcher to this actor.
    pub fn add_dispatcher(
        &mut self,
        dispatch_strategy: DispatchStrategy,
        dispatcher_id: u64,
        downstream_actors: OrderedActorLink,
        same_worker_node: bool,
    ) {
        self.downstreams.push(StreamActorDownstream {
            dispatch_strategy,
            dispatcher_id,
            actors: downstream_actors,
            same_worker_node,
        });
    }

    /// Build an actor after seal.
    pub fn build(&self) -> StreamActor {
        let dispatcher = self
            .downstreams
            .iter()
            .map(
                |StreamActorDownstream {
                     dispatch_strategy,
                     dispatcher_id,
                     actors,
                     same_worker_node: _,
                 }| Dispatcher {
                    downstream_actor_id: actors.as_global_ids(),
                    r#type: dispatch_strategy.r#type,
                    column_indices: dispatch_strategy.column_indices.clone(),
                    // will be filled later by stream manager
                    hash_mapping: None,
                    dispatcher_id: *dispatcher_id,
                },
            )
            .collect_vec();

        StreamActor {
            actor_id: self.actor_id.as_global_id(),
            fragment_id: self.fragment_id.as_global_id(),
            nodes: Some(self.nodes.deref().clone()),
            dispatcher,
            upstream_actor_id: self
                .upstreams
                .iter()
                .flat_map(|(_, StreamActorUpstream { actors, .. })| actors.0.iter().copied())
                .map(|x| x.as_global_id())
                .collect(), // TODO: store each upstream separately
            colocated_upstream_actor_id: if self.chain_same_worker_node {
                if self.upstreams.is_empty() {
                    None
                } else {
                    Some(ColocatedActorId {
                        id: self
                            .upstreams
                            .values()
                            .exactly_one()
                            .unwrap()
                            .actors
                            .as_global_ids()
                            .into_iter()
                            .exactly_one()
                            .unwrap(),
                    })
                }
            } else if self.upstreams.values().any(|u| u.same_worker_node) {
                Some(ColocatedActorId {
                    id: self
                        .upstreams
                        .values()
                        .filter(|x| x.same_worker_node)
                        .exactly_one()
                        .unwrap()
                        .actors
                        .as_global_ids()
                        .into_iter()
                        .exactly_one()
                        .unwrap(),
                })
            } else {
                None
            },
            vnode_bitmap: None,
            // To be filled by `StreamGraphBuilder::build`
            mview_definition: "".to_owned(),
        }
    }
}

/// [`StreamGraphBuilder`] build a stream graph. It injects some information to achieve
/// dependencies. See `build_inner` for more details.
#[derive(Default)]
struct StreamGraphBuilder {
    actor_builders: BTreeMap<GlobalActorId, StreamActorBuilder>,
}

impl StreamGraphBuilder {
    /// Insert new generated actor.
    pub fn add_actor(
        &mut self,
        actor_id: GlobalActorId,
        fragment_id: GlobalFragmentId,
        node: Arc<StreamNode>,
    ) {
        self.actor_builders.insert(
            actor_id,
            StreamActorBuilder::new(actor_id, fragment_id, node),
        );
    }

    /// Add dependency between two connected node in the graph.
    pub fn add_link(
        &mut self,
        upstream_fragment_id: GlobalFragmentId,
        upstream_actor_ids: &[GlobalActorId],
        downstream_actor_ids: &[GlobalActorId],
        edge: StreamFragmentEdge,
    ) {
        let exchange_operator_id = edge.link_id;
        let same_worker_node = edge.same_worker_node;
        let dispatch_strategy = edge.dispatch_strategy.unwrap();
        // We can't use the exchange operator id directly as the dispatch id, because an exchange
        // could belong to more than one downstream in DAG.
        // We can use downstream fragment id as an unique id for dispatcher.
        // In this way we can ensure the dispatchers of `StreamActor` would have different id,
        // even though they come from the same exchange operator.
        let dispatch_id = edge.downstream_id as u64;

        if dispatch_strategy.get_type().unwrap() == DispatcherType::NoShuffle {
            assert_eq!(
                upstream_actor_ids.len(),
                downstream_actor_ids.len(),
                "mismatched length when processing no-shuffle exchange: {:?} -> {:?} on exchange {}",
                upstream_actor_ids,
                downstream_actor_ids,
                exchange_operator_id
            );

            // update 1v1 relationship
            upstream_actor_ids
                .iter()
                .zip_eq(downstream_actor_ids.iter())
                .for_each(|(upstream_id, downstream_id)| {
                    self.actor_builders
                        .get_mut(upstream_id)
                        .unwrap()
                        .add_dispatcher(
                            dispatch_strategy.clone(),
                            dispatch_id,
                            OrderedActorLink(vec![*downstream_id]),
                            same_worker_node,
                        );

                    self.actor_builders
                        .get_mut(downstream_id)
                        .unwrap()
                        .upstreams
                        .try_insert(
                            exchange_operator_id,
                            StreamActorUpstream {
                                actors: OrderedActorLink(vec![*upstream_id]),
                                fragment_id: upstream_fragment_id,
                                same_worker_node,
                            },
                        )
                        .unwrap_or_else(|_| {
                            panic!(
                                "duplicated exchange input {} for no-shuffle actors {:?} -> {:?}",
                                exchange_operator_id, upstream_id, downstream_id
                            )
                        });
                });

            return;
        }

        // otherwise, make m * n links between actors.

        assert!(
            !same_worker_node,
            "same_worker_node only applies to 1v1 dispatchers."
        );

        // update actors to have dispatchers, link upstream -> downstream.
        upstream_actor_ids.iter().for_each(|upstream_id| {
            self.actor_builders
                .get_mut(upstream_id)
                .unwrap()
                .add_dispatcher(
                    dispatch_strategy.clone(),
                    dispatch_id,
                    OrderedActorLink(downstream_actor_ids.to_vec()),
                    same_worker_node,
                );
        });

        // update actors to have upstreams, link downstream <- upstream.
        downstream_actor_ids.iter().for_each(|downstream_id| {
            self.actor_builders
                .get_mut(downstream_id)
                .unwrap()
                .upstreams
                .try_insert(
                    exchange_operator_id,
                    StreamActorUpstream {
                        actors: OrderedActorLink(upstream_actor_ids.to_vec()),
                        fragment_id: upstream_fragment_id,
                        same_worker_node,
                    },
                )
                .unwrap_or_else(|_| {
                    panic!(
                        "duplicated exchange input {} for actors {:?} -> {:?}",
                        exchange_operator_id, upstream_actor_ids, downstream_actor_ids
                    )
                });
        });
    }

    /// Build final stream DAG with dependencies with current actor builders.
    #[allow(clippy::type_complexity)]
    pub fn build(
        self,
        ctx: &CreateStreamingJobContext,
    ) -> MetaResult<HashMap<GlobalFragmentId, Vec<StreamActor>>> {
        let mut graph: HashMap<GlobalFragmentId, Vec<StreamActor>> = HashMap::new();

        for builder in self.actor_builders.values() {
            let mut actor = builder.build();
            let upstream_actors = builder
                .upstreams
                .iter()
                .map(|(id, StreamActorUpstream { actors, .. })| (*id, actors.clone()))
                .collect();
            let upstream_fragments = builder
                .upstreams
                .iter()
                .map(|(id, StreamActorUpstream { fragment_id, .. })| (*id, *fragment_id))
                .collect();
            let stream_node =
                self.build_inner(actor.get_nodes()?, &upstream_actors, &upstream_fragments)?;

            actor.nodes = Some(stream_node);
            actor.mview_definition = ctx.streaming_definition.clone();

            graph.entry(builder.fragment_id()).or_default().push(actor);
        }
        Ok(graph)
    }

    /// Build stream actor inside, two works will be done:
    /// 1. replace node's input with [`MergeNode`] if it is `ExchangeNode`, and swallow
    /// mergeNode's input.
    /// 2. ignore root node when it's `ExchangeNode`.
    /// 3. replace node's `ExchangeNode` input with [`MergeNode`] and resolve its upstream actor
    /// ids if it is a `ChainNode`.
    fn build_inner(
        &self,
        stream_node: &StreamNode,
        upstream_actors: &HashMap<u64, OrderedActorLink>,
        upstream_fragments: &HashMap<u64, GlobalFragmentId>,
    ) -> MetaResult<StreamNode> {
        match stream_node.get_node_body()? {
            NodeBody::Exchange(_) => {
                panic!("ExchangeNode should be eliminated from the top of the plan node when converting fragments to actors: {:#?}", stream_node)
            }
            NodeBody::Chain(_) => Ok(self.resolve_chain_node(stream_node)?),
            _ => {
                let mut new_stream_node = stream_node.clone();

                for (input, new_input) in stream_node
                    .input
                    .iter()
                    .zip_eq(new_stream_node.input.iter_mut())
                {
                    *new_input = match input.get_node_body()? {
                        NodeBody::Exchange(e) => {
                            assert!(!input.get_fields().is_empty());
                            StreamNode {
                                input: vec![],
                                stream_key: input.stream_key.clone(),
                                node_body: Some(NodeBody::Merge(MergeNode {
                                    upstream_actor_id: upstream_actors
                                        .get(&input.get_operator_id())
                                        .expect("failed to find upstream actor id for given exchange node").as_global_ids(),
                                    upstream_fragment_id: upstream_fragments.get(&input.get_operator_id()).unwrap().as_global_id(),
                                    upstream_dispatcher_type: e.get_strategy()?.r#type,
                                    fields: input.get_fields().clone(),
                                })),
                                fields: input.get_fields().clone(),
                                operator_id: input.operator_id,
                                identity: "MergeExecutor".to_string(),
                                append_only: input.append_only,
                            }
                        }
                        _ => self.build_inner(input, upstream_actors, upstream_fragments)?,
                    }
                }
                Ok(new_stream_node)
            }
        }
    }

    /// Resolve the chain node, only rewrite the schema of input `MergeNode`.
    fn resolve_chain_node(&self, stream_node: &StreamNode) -> MetaResult<StreamNode> {
        let NodeBody::Chain(chain_node) = stream_node.get_node_body().unwrap() else {
            unreachable!()
        };
        let input = stream_node.get_input();
        assert_eq!(input.len(), 2);

        let merge_node = &input[0];
        assert_matches!(merge_node.node_body, Some(NodeBody::Merge(_)));
        let batch_plan_node = &input[1];
        assert_matches!(batch_plan_node.node_body, Some(NodeBody::BatchPlan(_)));

        let chain_input = vec![
            StreamNode {
                input: vec![],
                stream_key: merge_node.stream_key.clone(),
                node_body: Some(NodeBody::Merge(MergeNode {
                    upstream_actor_id: vec![],
                    upstream_fragment_id: 0,
                    upstream_dispatcher_type: DispatcherType::NoShuffle as _,
                    fields: chain_node.upstream_fields.clone(),
                })),
                fields: chain_node.upstream_fields.clone(),
                operator_id: merge_node.operator_id,
                identity: "MergeExecutor".to_string(),
                append_only: stream_node.append_only,
            },
            batch_plan_node.clone(),
        ];

        Ok(StreamNode {
            input: chain_input,
            stream_key: stream_node.stream_key.clone(),
            node_body: Some(NodeBody::Chain(chain_node.clone())),
            operator_id: stream_node.operator_id,
            identity: "ChainExecutor".to_string(),
            fields: chain_node.upstream_fields.clone(),
            append_only: stream_node.append_only,
        })
    }
}

/// The mutable state when building actor graph.
struct BuildActorGraphState {
    /// The stream graph builder, to build streaming DAG.
    stream_graph_builder: StreamGraphBuilder,

    /// When converting fragment graph to actor graph, we need to know which actors belong to a
    /// fragment.
    fragment_actors: HashMap<GlobalFragmentId, Vec<GlobalActorId>>,

    /// The next local actor id to use.
    next_local_id: u32,

    /// The global actor id generator.
    actor_id_gen: GlobalActorIdGen,
}

impl BuildActorGraphState {
    /// Create an empty state with the given id generator.
    fn new(actor_id_gen: GlobalActorIdGen) -> Self {
        Self {
            stream_graph_builder: Default::default(),
            fragment_actors: Default::default(),
            next_local_id: 0,
            actor_id_gen,
        }
    }

    /// Get the next global actor id.
    fn next_actor_id(&mut self) -> GlobalActorId {
        let local_id = self.next_local_id;
        self.next_local_id += 1;

        self.actor_id_gen.to_global_id(local_id)
    }

    /// Finish the build and return the inner stream graph builder.
    fn finish(self) -> StreamGraphBuilder {
        assert_eq!(self.actor_id_gen.len, self.next_local_id);
        self.stream_graph_builder
    }
}

/// [`ActorGraphBuilder`] generates the proto for interconnected actors for a streaming pipeline.
pub struct ActorGraphBuilder {
    /// The pre-scheduled distribution for each fragment.
    // TODO: this is fake now and only the parallelism is used, we should also use the physical
    // distribution itself after scheduler refactoring.
    distributions: HashMap<GlobalFragmentId, Distribution>,

    /// The stream fragment graph.
    fragment_graph: StreamFragmentGraph,
}

impl ActorGraphBuilder {
    /// Create a new actor graph builder with the given "complete" graph. Returns an error if the
    /// graph is failed to be scheduled.
    pub fn new(
        complete_graph: CompleteStreamFragmentGraph,
        default_parallelism: u32,
    ) -> MetaResult<Self> {
        // TODO: use the real parallel units to generate real distribution.
        let fake_parallel_units = (0..default_parallelism).map(|id| ParallelUnit {
            id,
            worker_node_id: 0,
        });
        let distributions =
            schedule::Scheduler::new(fake_parallel_units, default_parallelism as usize)?
                .schedule(&complete_graph)?;

        // TODO: directly use the complete graph when building so that we can generalize the
        // processing logic for `Chain`s.
        let fragment_graph = complete_graph.into_inner();

        Ok(Self {
            distributions,
            fragment_graph,
        })
    }

    /// Build a stream graph by duplicating each fragment as parallel actors.
    pub async fn generate_graph<S>(
        &self,
        id_gen_manager: IdGeneratorManagerRef<S>,
        ctx: &mut CreateStreamingJobContext,
    ) -> MetaResult<BTreeMap<FragmentId, Fragment>>
    where
        S: MetaStore,
    {
        let actor_len = self
            .distributions
            .values()
            .map(|d| d.parallelism())
            .sum::<usize>() as u64;
        let id_gen = GlobalActorIdGen::new(&id_gen_manager, actor_len).await?;

        // Generate actors of the streaming plan
        let stream_graph = self.build_actor_graph(id_gen, ctx)?.finish().build(&*ctx)?;

        // Serialize the graph
        let stream_graph = stream_graph
            .into_iter()
            .map(|(fragment_id, actors)| {
                let fragment = self.fragment_graph.seal_fragment(fragment_id, actors);
                let fragment_id = fragment_id.as_global_id();
                (fragment_id, fragment)
            })
            .collect();

        Ok(stream_graph)
    }

    /// Build actor graph from fragment graph using topological order. Setup dispatcher in actor and
    /// generate actors by their parallelism.
    fn build_actor_graph(
        &self,
        id_gen: GlobalActorIdGen,
        ctx: &mut CreateStreamingJobContext,
    ) -> MetaResult<BuildActorGraphState> {
        let mut state = BuildActorGraphState::new(id_gen);

        // Use topological sort to build the graph from downstream to upstream. (The first fragment
        // popped out from the heap will be the top-most node in plan, or the sink in stream graph.)
        for fragment_id in self.fragment_graph.topo_order()? {
            // Build the actors corresponding to the fragment
            self.build_actor_graph_fragment(fragment_id, &mut state, ctx)?;
        }

        Ok(state)
    }

    fn build_actor_graph_fragment(
        &self,
        fragment_id: GlobalFragmentId,
        state: &mut BuildActorGraphState,
        ctx: &mut CreateStreamingJobContext,
    ) -> MetaResult<()> {
        let current_fragment = self
            .fragment_graph
            .get_fragment(fragment_id)
            .unwrap()
            .clone();

        // TODO: remove this after scheduler refactoring.
        let upstream_table_id = current_fragment
            .upstream_table_ids
            .iter()
            .at_most_one()
            .unwrap()
            .map(TableId::from);
        if let Some(upstream_table_id) = upstream_table_id {
            ctx.chain_fragment_upstream_table_map
                .insert(fragment_id.as_global_id(), upstream_table_id);
        }

        let distribution = &self.distributions[&fragment_id];
        let parallel_degree = distribution.parallelism() as u32;

        let node = Arc::new(current_fragment.node.unwrap());

        let actor_ids = (0..parallel_degree)
            .map(|_| state.next_actor_id())
            .collect_vec();

        for &actor_id in &actor_ids {
            state
                .stream_graph_builder
                .add_actor(actor_id, fragment_id, node.clone());
        }

        for (downstream_fragment_id, dispatch_edge) in
            self.fragment_graph.get_downstreams(fragment_id)
        {
            let downstream_actors = state
                .fragment_actors
                .get(downstream_fragment_id)
                .expect("downstream fragment not processed yet");

            state.stream_graph_builder.add_link(
                fragment_id,
                &actor_ids,
                downstream_actors,
                dispatch_edge.clone(),
            );
        }

        state
            .fragment_actors
            .try_insert(fragment_id, actor_ids)
            .unwrap_or_else(|_| panic!("fragment {:?} is already processed", fragment_id));

        Ok(())
    }
}

/// The fragment in the building phase, including the [`StreamFragment`] from the frontend and
/// several additional helper fields.
#[derive(Debug, Clone)]
pub struct BuildingFragment {
    /// The fragment structure from the frontend, with the global fragment ID.
    inner: StreamFragment,

    /// A clone of the internal tables in this fragment.
    internal_tables: Vec<Table>,

    /// The ID of the job if it's materialized in this fragment.
    table_id: Option<u32>,
}

impl BuildingFragment {
    /// Create a new [`BuildingFragment`] from a [`StreamFragment`]. The global fragment ID and
    /// global table IDs will be correctly filled with the given `id` and `table_id_gen`.
    fn new(
        id: GlobalFragmentId,
        fragment: StreamFragment,
        job: &StreamingJob,
        table_id_gen: GlobalTableIdGen,
    ) -> Self {
        let mut fragment = StreamFragment {
            fragment_id: id.as_global_id(),
            ..fragment
        };
        let internal_tables = Self::fill_internal_tables(&mut fragment, job, table_id_gen);
        let table_id = Self::fill_job(&mut fragment, job).then(|| job.id());

        Self {
            inner: fragment,
            internal_tables,
            table_id,
        }
    }

    /// Fill the information of the internal tables in the fragment.
    fn fill_internal_tables(
        fragment: &mut StreamFragment,
        job: &StreamingJob,
        table_id_gen: GlobalTableIdGen,
    ) -> Vec<Table> {
        let fragment_id = fragment.fragment_id;
        let mut internal_tables = Vec::new();

        visit_internal_tables(fragment, |table, table_type_name| {
            table.id = table_id_gen.to_global_id(table.id).as_global_id();
            table.schema_id = job.schema_id();
            table.database_id = job.database_id();
            table.name = generate_internal_table_name_with_type(
                &job.name(),
                fragment_id,
                table.id,
                table_type_name,
            );
            table.fragment_id = fragment_id;

            // Record the internal table.
            internal_tables.push(table.clone());
        });

        internal_tables
    }

    /// Fill the information of the job in the fragment.
    fn fill_job(fragment: &mut StreamFragment, job: &StreamingJob) -> bool {
        let table_id = job.id();
        let fragment_id = fragment.fragment_id;
        let mut has_table = false;

        visit_fragment(fragment, |node_body| match node_body {
            NodeBody::Materialize(materialize_node) => {
                materialize_node.table_id = table_id;

                // Fill the ID of the `Table`.
                let table = materialize_node.table.as_mut().unwrap();
                table.id = table_id;
                table.database_id = job.database_id();
                table.schema_id = job.schema_id();
                table.fragment_id = fragment_id;

                has_table = true;
            }
            NodeBody::Sink(sink_node) => {
                sink_node.table_id = table_id;

                has_table = true;
            }
            NodeBody::Dml(dml_node) => {
                dml_node.table_id = table_id;
            }
            _ => {}
        });

        has_table
    }
}

impl Deref for BuildingFragment {
    type Target = StreamFragment;

    fn deref(&self) -> &Self::Target {
        &self.inner
    }
}

/// In-memory representation of a **Fragment** Graph, built from the [`StreamFragmentGraphProto`]
/// from the frontend.
#[derive(Default)]
pub struct StreamFragmentGraph {
    /// stores all the fragments in the graph.
    fragments: HashMap<GlobalFragmentId, BuildingFragment>,

    /// stores edges between fragments: upstream => downstream.
    downstreams: HashMap<GlobalFragmentId, HashMap<GlobalFragmentId, StreamFragmentEdge>>,

    /// stores edges between fragments: downstream -> upstream.
    upstreams: HashMap<GlobalFragmentId, HashMap<GlobalFragmentId, StreamFragmentEdge>>,

    /// Dependent relations of this job.
    dependent_relations: HashSet<TableId>,
}

impl StreamFragmentGraph {
    /// Create a new [`StreamFragmentGraph`] from the given [`StreamFragmentGraphProto`], with all
    /// global IDs correctly filled.
    pub async fn new<S: MetaStore>(
        proto: StreamFragmentGraphProto,
        id_gen: IdGeneratorManagerRef<S>,
        job: &StreamingJob,
    ) -> MetaResult<Self> {
        let fragment_id_gen =
            GlobalFragmentIdGen::new(&id_gen, proto.fragments.len() as u64).await?;
        let table_id_gen = GlobalTableIdGen::new(&id_gen, proto.table_ids_cnt as u64).await?;

        // Create nodes.
        let fragments: HashMap<_, _> = proto
            .fragments
            .into_iter()
            .map(|(id, fragment)| {
                let id = fragment_id_gen.to_global_id(id);
                let fragment = BuildingFragment::new(id, fragment, job, table_id_gen);
                (id, fragment)
            })
            .collect();

        assert_eq!(
            fragments
                .values()
                .map(|f| f.internal_tables.len() as u32)
                .sum::<u32>(),
            proto.table_ids_cnt
        );

        // Create edges.
        let mut downstreams = HashMap::new();
        let mut upstreams = HashMap::new();

        for edge in proto.edges {
            let upstream_id = fragment_id_gen.to_global_id(edge.upstream_id);
            let downstream_id = fragment_id_gen.to_global_id(edge.downstream_id);

            upstreams
                .entry(downstream_id)
                .or_insert_with(HashMap::new)
                .try_insert(
                    upstream_id,
                    StreamFragmentEdge {
                        upstream_id: upstream_id.as_global_id(),
                        downstream_id: downstream_id.as_global_id(),
                        ..edge.clone()
                    },
                )
                .unwrap();
            downstreams
                .entry(upstream_id)
                .or_insert_with(HashMap::new)
                .try_insert(
                    downstream_id,
                    StreamFragmentEdge {
                        upstream_id: upstream_id.as_global_id(),
                        downstream_id: downstream_id.as_global_id(),
                        ..edge
                    },
                )
                .unwrap();
        }

        // Note: Here we directly use the field `dependent_table_ids` in the proto (resolved in
        // frontend), instead of visiting the graph ourselves. Note that for creating table with a
        // connector, the source itself is NOT INCLUDED in this list.
        let dependent_relations = proto
            .dependent_table_ids
            .iter()
            .map(TableId::from)
            .collect();

        Ok(Self {
            fragments,
            downstreams,
            upstreams,
            dependent_relations,
        })
    }

    /// Generate topological order of the fragments in this graph. Returns error if the graph is not
    /// a DAG and topological sort can not be done.
    ///
    /// The first fragment popped out from the heap will be the top-most node, or the
    /// `Sink`/`Materialize` in stream graph.
    fn topo_order(&self) -> MetaResult<Vec<GlobalFragmentId>> {
        let mut topo = Vec::new();
        let mut downstream_cnts = HashMap::new();

        // Iterate all fragments
        for fragment_id in self.fragments.keys() {
            // Count how many downstreams we have for a given fragment
            let downstream_cnt = self.get_downstreams(*fragment_id).len();
            if downstream_cnt == 0 {
                topo.push(*fragment_id);
            } else {
                downstream_cnts.insert(*fragment_id, downstream_cnt);
            }
        }

        let mut i = 0;
        while let Some(&fragment_id) = topo.get(i) {
            i += 1;
            // Find if we can process more fragments
            for upstream_id in self.get_upstreams(fragment_id).keys() {
                let downstream_cnt = downstream_cnts.get_mut(upstream_id).unwrap();
                *downstream_cnt -= 1;
                if *downstream_cnt == 0 {
                    downstream_cnts.remove(upstream_id);
                    topo.push(*upstream_id);
                }
            }
        }

        if !downstream_cnts.is_empty() {
            // There are fragments that are not processed yet.
            bail!("graph is not a DAG");
        }

        Ok(topo)
    }

    /// Seal a [`StreamFragment`] from the graph into a [`Fragment`], which will be further used to
    /// build actors, schedule, and persist into meta store.
    fn seal_fragment(&self, id: GlobalFragmentId, actors: Vec<StreamActor>) -> Fragment {
        let BuildingFragment {
            inner,
            internal_tables,
            table_id,
        } = self.fragments.get(&id).unwrap().to_owned();

        let distribution_type = if inner.is_singleton {
            FragmentDistributionType::Single
        } else {
            FragmentDistributionType::Hash
        } as i32;

        let state_table_ids = internal_tables
            .iter()
            .map(|t| t.id)
            .chain(table_id)
            .collect();

        let upstream_fragment_ids = self
            .get_upstreams(id)
            .keys()
            .map(|id| id.as_global_id())
            .collect();

        Fragment {
            fragment_id: inner.fragment_id,
            fragment_type_mask: inner.fragment_type_mask,
            distribution_type,
            actors,
            // Will be filled in `Scheduler::schedule` later.
            vnode_mapping: None,
            state_table_ids,
            upstream_fragment_ids,
        }
    }

    /// Retrieve the internal tables map of the whole graph.
    pub fn internal_tables(&self) -> HashMap<u32, Table> {
        let mut tables = HashMap::new();
        for fragment in self.fragments.values() {
            for table in &fragment.internal_tables {
                tables
                    .try_insert(table.id, table.clone())
                    .unwrap_or_else(|_| panic!("duplicated table id `{}`", table.id));
            }
        }
        tables
    }

    /// Returns the fragment id where the table is materialized.
    pub fn table_fragment_id(&self) -> FragmentId {
        self.fragments
            .values()
            .filter(|b| b.table_id.is_some())
            .map(|b| b.fragment_id)
            .exactly_one()
            .expect("require exactly 1 materialize/sink node when creating the streaming job")
    }

    pub fn dependent_relations(&self) -> &HashSet<TableId> {
        &self.dependent_relations
    }

    fn get_fragment(&self, fragment_id: GlobalFragmentId) -> Option<&StreamFragment> {
        self.fragments.get(&fragment_id).map(|b| b.deref())
    }

    fn get_downstreams(
        &self,
        fragment_id: GlobalFragmentId,
    ) -> &HashMap<GlobalFragmentId, StreamFragmentEdge> {
        self.downstreams.get(&fragment_id).unwrap_or(&EMPTY_HASHMAP)
    }

    fn get_upstreams(
        &self,
        fragment_id: GlobalFragmentId,
    ) -> &HashMap<GlobalFragmentId, StreamFragmentEdge> {
        self.upstreams.get(&fragment_id).unwrap_or(&EMPTY_HASHMAP)
    }

    fn edges(
        &self,
    ) -> impl Iterator<Item = (GlobalFragmentId, GlobalFragmentId, &StreamFragmentEdge)> {
        self.downstreams
            .iter()
            .flat_map(|(&from, tos)| tos.iter().map(move |(&to, edge)| (from, to, edge)))
    }
}

static EMPTY_HASHMAP: LazyLock<HashMap<GlobalFragmentId, StreamFragmentEdge>> =
    LazyLock::new(HashMap::new);

/// A wrapper of [`StreamFragmentGraph`] that contains the information of existing fragments, which
/// is connected to the graph's top-most or bottom-most fragments.
///
/// For example, if we're going to build a mview on an existing mview, the upstream fragment
/// containing the `Materialize` node will be included in this structure.
pub struct CompleteStreamFragmentGraph {
    /// The fragment graph of this streaming job.
    graph: StreamFragmentGraph,

    /// The required information of existing fragments.
    existing_fragments: HashMap<GlobalFragmentId, Fragment>,

    /// Extra edges between existing fragments and the building fragments.
    downstreams: HashMap<GlobalFragmentId, HashMap<GlobalFragmentId, DispatcherType>>,

    /// Extra edges between existing fragments and the building fragments.
    #[expect(dead_code)]
    upstreams: HashMap<GlobalFragmentId, HashMap<GlobalFragmentId, DispatcherType>>,
}

impl CompleteStreamFragmentGraph {
    /// Create a new [`CompleteStreamFragmentGraph`] with empty existing fragments, i.e., there's no
    /// upstream mviews.
    #[cfg(test)]
    pub fn for_test(graph: StreamFragmentGraph) -> Self {
        Self {
            graph,
            existing_fragments: Default::default(),
            downstreams: Default::default(),
            upstreams: Default::default(),
        }
    }

    /// Create a new [`CompleteStreamFragmentGraph`] for MV-on-MV. Returns an error if the upstream
    /// `Matererialize` is failed to resolve.
    pub fn new(
        graph: StreamFragmentGraph,
        upstream_mview_fragments: HashMap<TableId, Fragment>,
    ) -> MetaResult<Self> {
        let mut downstreams = HashMap::new();
        let mut upstreams = HashMap::new();

        for (&id, fragment) in &graph.fragments {
            for &upstream_table_id in &fragment.upstream_table_ids {
                let mview_fragment = upstream_mview_fragments
                    .get(&TableId::new(upstream_table_id))
                    .context("upstream materialized view fragment not found")?;
                let mview_id = GlobalFragmentId::new(mview_fragment.fragment_id);

                // We always use `NoShuffle` for the exchange between the upstream `Materialize` and
                // the downstream `Chain` of the new materialized view.
                let dt = DispatcherType::NoShuffle;

                downstreams
                    .entry(mview_id)
                    .or_insert_with(HashMap::new)
                    .try_insert(id, dt)
                    .unwrap();
                upstreams
                    .entry(id)
                    .or_insert_with(HashMap::new)
                    .try_insert(mview_id, dt)
                    .unwrap();
            }
        }

        let existing_fragments = upstream_mview_fragments
            .into_values()
            .map(|f| (GlobalFragmentId::new(f.fragment_id), f))
            .collect();

        Ok(Self {
            graph,
            existing_fragments,
            downstreams,
            upstreams,
        })
    }

    /// Returns the dispatcher types of all edges in the complete graph.
    fn dispatch_edges(
        &self,
    ) -> impl Iterator<Item = (GlobalFragmentId, GlobalFragmentId, DispatcherType)> + '_ {
        self.graph
            .edges()
            .map(|(from, to, edge)| {
                let dt = edge.get_dispatch_strategy().unwrap().get_type().unwrap();
                (from, to, dt)
            })
            .chain(
                self.downstreams
                    .iter()
                    .flat_map(|(&from, tos)| tos.iter().map(move |(&to, &dt)| (from, to, dt))),
            )
    }

    /// Returns the distribution of the existing fragments.
    fn existing_distribution(&self) -> HashMap<GlobalFragmentId, Distribution> {
        self.existing_fragments
            .iter()
            .map(|(&id, f)| {
                let dist = match f.get_distribution_type().unwrap() {
                    FragmentDistributionType::Unspecified => unreachable!(),
                    FragmentDistributionType::Single => Distribution::Singleton,
                    FragmentDistributionType::Hash => {
                        let mapping =
                            ParallelUnitMapping::from_protobuf(f.get_vnode_mapping().unwrap());
                        Distribution::Hash(mapping)
                    }
                };
                (id, dist)
            })
            .collect()
    }

    /// Consumes this complete graph and returns the inner graph.
    // TODO: remove this after scheduler refactoring.
    pub fn into_inner(self) -> StreamFragmentGraph {
        self.graph
    }
}

/// A utility for visiting and mutating the [`NodeBody`] of the [`StreamNode`]s in a
/// [`StreamFragment`] recursively.
pub fn visit_fragment<F>(fragment: &mut StreamFragment, mut f: F)
where
    F: FnMut(&mut NodeBody),
{
    fn visit_inner<F>(stream_node: &mut StreamNode, f: &mut F)
    where
        F: FnMut(&mut NodeBody),
    {
        f(stream_node.node_body.as_mut().unwrap());
        for input in &mut stream_node.input {
            visit_inner(input, f);
        }
    }

    visit_inner(fragment.node.as_mut().unwrap(), &mut f)
}

/// Visit the internal tables of a [`StreamFragment`].
fn visit_internal_tables<F>(fragment: &mut StreamFragment, mut f: F)
where
    F: FnMut(&mut Table, &'static str),
{
    macro_rules! always {
        ($table:expr, $name:expr) => {{
            let table = $table
                .as_mut()
                .unwrap_or_else(|| panic!("internal table {} should always exist", $name));
            f(table, $name);
        }};
    }

    #[allow(unused_macros)]
    macro_rules! optional {
        ($table:expr, $name:expr) => {
            if let Some(table) = &mut $table {
                f(table, $name);
            }
        };
    }

    visit_fragment(fragment, |body| {
        match body {
            // Join
            NodeBody::HashJoin(node) => {
                // TODO: make the degree table optional
                always!(node.left_table, "HashJoinLeft");
                always!(node.left_degree_table, "HashJoinDegreeLeft");
                always!(node.right_table, "HashJoinRight");
                always!(node.right_degree_table, "HashJoinDegreeRight");
            }
            NodeBody::DynamicFilter(node) => {
                always!(node.left_table, "DynamicFilterLeft");
                always!(node.right_table, "DynamicFilterRight");
            }

            // Aggregation
            NodeBody::HashAgg(node) => {
                assert_eq!(node.agg_call_states.len(), node.agg_calls.len());
                always!(node.result_table, "HashAggResult");
                for state in &mut node.agg_call_states {
                    if let agg_call_state::Inner::MaterializedInputState(s) =
                        state.inner.as_mut().unwrap()
                    {
                        always!(s.table, "HashAgg");
                    }
                }
            }
            NodeBody::GlobalSimpleAgg(node) => {
                assert_eq!(node.agg_call_states.len(), node.agg_calls.len());
                always!(node.result_table, "GlobalSimpleAggResult");
                for state in &mut node.agg_call_states {
                    if let agg_call_state::Inner::MaterializedInputState(s) =
                        state.inner.as_mut().unwrap()
                    {
                        always!(s.table, "GlobalSimpleAgg");
                    }
                }
            }

            // Top-N
            NodeBody::AppendOnlyTopN(node) => {
                always!(node.table, "AppendOnlyTopN");
            }
            NodeBody::TopN(node) => {
                always!(node.table, "TopN");
            }
            NodeBody::AppendOnlyGroupTopN(node) => {
                always!(node.table, "AppendOnlyGroupTopN");
            }
            NodeBody::GroupTopN(node) => {
                always!(node.table, "GroupTopN");
            }

            // Source
            NodeBody::Source(node) => {
                if let Some(source) = &mut node.source_inner {
                    always!(source.state_table, "Source");
                }
            }
            NodeBody::Now(node) => {
                always!(node.state_table, "Now");
            }

            // Shared arrangement
            NodeBody::Arrange(node) => {
                always!(node.table, "Arrange");
            }

            // Note: add internal tables for new nodes here.
            _ => {}
        }
    })
}
